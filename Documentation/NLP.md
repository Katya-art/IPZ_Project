### Що таке NPL?
Natural Language Processing, як правило, скорочена як NPI - це галузь штучного інтелекту, яка займається взаємодією між комп'ютерами та людьми за допомогою природної мови.
Кінцева мета NPL - читати, розшифровувати, розуміти та осмислювати людські мови.
Більшість методик NPL покладаються на машинне навчання, щоб отримувати значення з людських мов.

### Для чого використовується NPL?
NPL застосовується у:
•	програмах перекладу мов, таких як Google Translate;
•	пекстових процесорах, які використовують NLP для перевірки граматичної точності текстів. Наприклад Microsoft Word та Grammarly;
•	програмах інтерактивної голосової відповіді (IVR), що використовуються в телефонних центрах для відповіді на запити певних користувачів.
•	персональних програмах-помічниках, таких як OK Google, Siri, Cortana та Alexa.

### Чому вирішувати задачі NPL складно?
Формулювання завдань не дуже складні, проте самі завдання зовсім не є простими, тому що ми працюємо з природною мовою. Явища полісемії (багатозначні слова мають загальний вихідний сенс) і омонімії (різні за змістом слова вимовляються і пишуться однаково) характерні для будь-якого природної мови.

Полісемія: зупинка (процес або будівля), стіл (організація або об'єкт), дятел (птах або людина).
Омонімія: ключ, цибуля, замок, піч.

Іншим класичним прикладом складності мови є займенникова анафора. Наприклад, нехай нам дано текст «Двірник дві години прибирав сніг, він був незадоволений». Займенник «він» може ставитися як до двірника, так і до снігу. По контексту ми легко зрозуміємо, що він - це двірник, а не сніг. Але домогтися, щоб комп'ютер це теж легко розумів, непросто. Завдання займенникової анафори і зараз вирішено не дуже добре, тривають активні спроби поліпшити якість рішень.
Ще одна додаткова складність - це еліпсис. Наприклад, «Петрик з'їв зелене яблуко, а Маша - червоне». Ми розуміємо, що Маша з'їла червоне яблуко. Проте, домогтися, щоб машина теж це зрозуміла, непросто. Зараз завдання відновлення вирішується на крихітних корпусах (кілька сотень пропозицій), і на них якість повного відновлення відверто слабка (близько 0.5). Зрозуміло, що для практичних застосувань така якість нікуди не годиться. 

### Як працює обробка природних мов?
NLP тягне за собою застосування алгоритмів для ідентифікації та вилучення правил природної мови таким чином, що неструктуровані мовні дані перетворювались у форму, зрозумілу комп'ютерам.
Сирий текст не підходить для навчання машин. Потрібно конвертувати текст у вектори – набори цифр. Цей процес називають виділенням ознак.

Мішок слів – техніка виділення ознак, яка описує входження слів у текст. 

Для використання моделі необхідно визначити словник відомих слів, які називають токенами, а також обрати ступінь їхньої присутності. 

#### Приклад

Розглянемо створення моделі, використовуючи приклад для наочності.

1.	Загрузка даних
   
      У нас є дані:
      
      I like this movie, it's funny.  
      I hate this movie.  
      This was awesome! I like it.  
      Nice one. I love it.  
      Загружаємо їх у вигляді масиву: ["I like this movie, it's funny.", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']  

2.	Визначаємо словник

Збираємо усі унікальні слова, ігноруючи реєстр, пунктуацію, токени з одного символу.

3.	Створюємо вектори документа

Перетворюємо сирий текст у набір цифр, щоб використовувати їх як вхідні дані для моделі машинного навчання.
Відмічаємо наявність слів (1 – є, 0 – немає)
Результат:

|      | awesome | funny | hate | it   | like | love | movie | nice | one  | this | was  |
| ---: |--------:| -----:|-----:| ---: | ---: | ---: | ----: | ---: | ---: | ---: | ---: |
| 0    | 0       | 1     | 0    | 1    | 1    | 0    | 1     | 0    | 0    | 1    | 0    |
| 1    | 0       | 0     | 1    | 0    | 0    | 0    | 1     | 0    | 0    | 1    | 0    |
| 2    | 1       | 0     | 0    | 1    | 1    | 0    | 0     | 0    | 0    | 1    | 1    |
| 3    | 0       | 0     | 0    | 1    | 0    | 1    | 0     | 1    | 1    | 0    | 0    |

Із словником росте вектор документу. У прикладі довжина вектора дорівнює кількості відомих слів. Якщо вектор складається з тисяч або мільйонів елементів, кожен документ може містити лише меншу частину слів їх словника. Виходить вектор з багатьма нулями, який вимагає багато пам’яті та ресурсів.

Можна зменшити кількість слів, щоб зменшити вимоги до обчислювальних ресурсів.
Наприклад, викинути стоп-слова, привести слова до базових форм і виправити неправильно написані слова.
Інший спосіб – використання згрупованих слів, інакше кажучи, N-грам (N – кількість згрупованих слів). У модель потрапляють лише ті, що фігурують у корпусі.

#### Приклад:

Речення: The office building is open today

Його біграми (сполучення двох слів):
  * the office
  * office building
  * building is
  * is open
  * open today

Ми оцінили наявність слів, але також можна рахувати їх кількість у документі та частоту відносно загальної кількості.
Коли текст буде надано, комп'ютер буде використовувати алгоритми, щоб витягнути значення, пов'язане з кожним реченням, та зібрати з них суттєві дані.
Іноді комп'ютер може не зрозуміти значення речення, що призводить до незрозумілих результатів.

### Які методи використовуються в НЛП?
Синтаксичний аналіз та семантичний аналіз - основні прийоми, що використовуються для виконання завдань з обробки природних мов.
1. Синтаксис
Синтаксис позначає розташування слів у реченні таким чином, що вони мають граматичний зміст.
У NLP синтаксичний аналіз використовується для оцінки того, як природна мова узгоджується з граматичними правилами.
Комп'ютерні алгоритми використовуються для застосування граматичних правил до групи слів та отримання значень від них.
Ось кілька синтаксичних прийомів:
o	Лематизація: це тягне за собою зведення різних складних форм слова в єдину форму для легкого аналізу.
o	Морфологічна сегментація: передбачає поділ слів на окремі одиниці, що називаються морфемами.
o	Сегментація слова: передбачає поділ великого фрагмента безперервного тексту на окремі одиниці.
o	Позначення частин мови: передбачає визначення частини мови для кожного слова.
o	Розбір: Він передбачає проведення граматичного аналізу поданого речення.
o	Позначення кінця речення: передбачає розміщення меж речення на великому фрагменті тексту.
o	Стовбування: Це передбачає вирізання схильних слів до їх кореневої форми.
2. Семантика
Семантика позначає значення, яке передається текстом. Семантичний аналіз - один із складних аспектів обробки природних мов, який ще не був повністю вирішений.
Він передбачає застосування комп’ютерних алгоритмів для розуміння значення та інтерпретації слів та структури речень.
Ось деякі методи семантичного аналізу:
o	Розпізнавання іменованої сутності (NER): включає визначення частин тексту, які можна ідентифікувати та класифікувати за попередньо заданими групами. Приклади таких груп включають імена людей та назви місць.
o	Розбіжність сенсу у слові: передбачає надання значення слову на основі контексту.
o	Генерація природних мов: передбачає використання баз даних для отримання смислових намірів та перетворення їх на людську мову.

### Бібліотеки для NPL

Подальшу інформацію було взято з http://neerc.ifmo.ru/wiki/index.php?title=Обработка_естественного_языка#cite_note-8

#### NLTK (https://www.nltk.org)
NLTK(Natural Language Toolkit) - це провідна платформа для побудови програм Python для роботи з даними людської мови. Вона надає прості у використанні інтерфейси для понад 50 корпоративних та лексичних ресурсів, таких як WordNet, а також набір бібліотек для обробки тексту для класифікації, токенізації, стримування, розмітки тегів, розбору та семантичних міркувань.

Плюси:
•	Найвідоміша і повна по функціоналу бібліотека для NLP;
•	Велика кількість сторонніх розширень;
•	Швидка токенізація пропозицій;
•	Підтримує безліч мов.

Мінуси:
•	повільна;
•	Складна в вивченні і використанні;
•	Працює з рядками;
•	Не використовує нейронні мережі;
•	Не має  вбудованих векторів слів.


#### spaCy(https://spacy.io)
Бібліотека, написана на мові Cypthon, позиціонується як найшвидша NLP бібліотека. Має безліч можливостей, в тому числі, розбір залежностей на основі міток, розпізнавання іменованих сутностей, позначення частин мови, вектори розстановки слів. Не підтримує російську мову.

Плюси:
•	Найшвидша бібліотека для NLP;
•	Проста у вивченні і використанні;
•	Працює з об'єктами, а не рядками;
•	Є вбудовані вектори слів;
•	Використовує нейронні мережі для тренування моделей.

Мінуси:
•	Менш гнучка в порівнянні з NLTK;
•	Токенізація пропозицій повільніше, ніж в NLTK;
•	Підтримує невелику кількість мов.

#### scikit-learn(https://scikit-learn.org/stable/)
Бібліотека scikit-learn надає реалізацію цілого ряду алгоритмів для навчання з учителем і навчання без учителя через інтерфейс для Python. Побудована поверх SciPy. Орієнтована в першу чергу на моделювання даних, має досить функцій, щоб використовуватися для NLP в зв'язці з іншими бібліотеками.

Плюси:
•	Велика кількість алгоритмів для побудови моделей;
•	Містить функції для роботи з Bag-of-Words моделлю;
•	Гарна документація.

Мінуси:
•	Поганий препроцессінг, що змушує використовувати її в зв'язці з іншою бібліотекою (наприклад, NLTK);
•	Не використовує нейронні мережі для препроцессінга тексту.

#### gensim(https://radimrehurek.com/gensim/)
Python бібліотека для моделювання, тематичного моделювання документів і вилучення подібності для великих корпусів. У gensim реалізовані популярні NLP алгоритми, наприклад, word2vec. Більшість реалізацій можуть використовувати кілька ядер.

Плюси:
•	Працює з великими датасетами;
•	Підтримує глибоке навчання;
•	word2vec, tf-idf vectorization, document2vec.

Мінуси:
•	Заточена під моделі без вчителя;
•	Не містить достатнього функціоналу, необхідного для NLP, що змушує використовувати її разом з іншими бібліотеками.


Балто-слов'янські мови мають складну морфологію, що може погіршити якість обробки тексту, а також обмежити використання ряду бібліотек. Для роботи зі специфічною російської морфологією можна використовувати, наприклад, морфологічний аналізатор pymorphy2 і бібліотеку для пошуку і вилучення іменованих сутностей Natasha 


 
 ### Сервіси, що реалізують NLP
 
 #### Textrazor
 
   В мережі існують певні сервіси, наприклад [textrazor.com](https://www.textrazor.com), який надає можливість безкоштовно протестувати можливості NLP (demo), та має кілька функціональних переваг. Подальшу інформацію було взято з <https://www.textrazor.com/technology>

* TextRazor використовує сучасні методи обробки природних мов та штучного інтелекту для аналізу, аналізу та вилучення семантичних метаданих із заданого вмісту.

* API TextRazor можна легко інтегрувати з будь-якою мовою, яка може надіслати HTTP-запит і проаналізувати відповідь JSON, завдяки чому можлива потужна аналітика тексту лише за допомогою декількох рядків коду. TextRazor дозволяє витягти будь-яку та всю необхідну інформацію в одному запиті, пов'язуючи витягнуті семантичні метадані, щоб спростити ідентифікацію складних шаблонів.

* Великі дані корисні лише в тому випадку, якщо користувацьке програмне забезпечення може йти в ногу з цим. TextRazor був розроблений з нуля для продуктивності. Написаний на сильно оптимізованому C ++, TextRazor здатний обробляти тисячі слів в секунду. Сервіс створений для автоматичного масштабування до мільярдів запитів у хмарі. 

* Стійка інфраструктура TextRazor побудована на хмарі Amazon Web Services і фізичному обладнанні. TextRazor розроблений для забезпечення високої доступності та послідовності продуктивності для аналізу тисяч, мільйонів або мільярдів щоденних документів.

* TextRazor дозволяє додавати назви продуктів, людей, компаній, спеціальні правила класифікації та вдосконалені мовні зразки. Інтегрований двигун Prolog дозволяє швидко поєднувати результати TextRazor з надійною логікою, призначеною для користувача. 

#### Наташа

Подальшу інформацію було взято з <https://habr.com/ru/post/349864/>

Для витягування слів з російського тексту рішень небагато. Є рішення у Томіта-парсері, але там інтеграція з Python незручна. Також з iPavlov є рішення, але імена не зводяться до нормальної форми. З витягуванням адрес або посилань на нормативні акти ще складніше.

Наташа – аналог Томіта-парсера для Python і набір готових правил для витягування імен, адрес, дат, сум грошей та інших сутностей. Можна додавати свої правила за допомогою Yargy-парсера.

Зараз є правила для витягування імен, адрес, дат і сум грошей.  Правила для назв організацій і географічних об’єктів нижчої якості. 

У 2016 році проводилося змагання factRuEval-2016 з витягування іменних сутностей. Найкраща F1-міра була більше 0.9. У Наташі результат 0.78 через проблеми з іноземними іменами та складними прізвищами. Для текстів з російськими іменами результат ~0.95.

Проблемою бібліотеки Наташа є обмеженість готовими правилами. Наприклад, не вдасться визначити ««1» липня 2018» як дату, бо в правилах не враховані лапки. Також не вдасться розібрати адресу без назви вулиці.
У таких випадках доводиться доповнювати готові правила та писати свої за допомогою Yargy-парсеру, бібліотеці, яка є основою Наташі.

Yargy-парсер – аналог яндексового Томіта-парсера для Python. Правила для витягування сутностей описуються за допомогою контекстно-вільних граматик і словників.
Граматики в Yargy записуються на спеціальному DSL-і. Застосовуються вони для таких задач як витягування дат в ISO-форматі («2018-03-04», «2014-04-28»).

У парсер вбудовано багато готових предикантів і є можливість додати свої. Для визначення морфології слів використовується pymorphy2.

Парсер також виконує інтерпрепацію, і, наприклад, замість «16 серпня» повертає Date(month=8, day=16). Результат роботи парсера – дерево розбору.
 
Для інтерпретації користувач розвішує на вузли дерева помітки.
 
У прикладі потрібно звести назви років, місяців і днів до числам. Ця процедура називається нормалізацією.
Наприклад, січень – 1, лютий – 2, березень – 3 і так далі.

Також у Yargy є механізм узгодження, наприклад, імен і прізвищ за родом, числом та відмінком.

Рішення Наташі під ліцензією MIT, тобто, можна вільно використовувати, модифікувати і так далі.

Недоліками Наташі є потреба вручну складати правила та повільна швидкість (наприклад, витягування імен у 10 разів повільніше ніж у Томіта-парсера). Також наявні помилки у стандартних правилах.

Адреса проекту на Github — <https://github.com/natasha>.

### ВЕСУМ-великий електронний словник української мови

Подальшу інформацію було взято з <https://r2u.org.ua/articles/vesum>

Бере початок з проекту ispell-uk, що його в 90-х роках створила група ентузіастів для перевірки орфографії української мови у відкритій ОС Linux. Багато років цей словник мав єдину функцію — перевіряти орфографію текстів. Але декілька років тому в інший відкритий проект, програму перевірки граматики та стилю LanguageTool, було додано модуль української мови

До проекту долучилася команда створення відкритого корпусу української мови БрУК

Для створення ВЕСУМ-а ми використали два найголовніших джерела: “Граматичний словник української літературної мови. Словозміна” (опублікований 2011 року й удоступнений на Лінгвістичному порталі (<http://www.mova.info/grmasl.aspx>) та “Словники України” онлайн (<http://lcorp.ulif.org.ua/dictua/>).

#### Зусилля, вкладені в проект, на виході дали без перебільшення унікальний словник:

Основні застосування:

  * налічує понад 285 тис. лем і постійно поповнюється

  * містить інформацію про відмінювання слів

  * подає нерекомендовані слова (активні дієприкметники, невдалі кальки тощо) та заміну для них

  * охоплює абревіатури та скорочення

  * містить інформацію про деякі альтернативні правописні варіанти (дає змогу аналізувати тексти, написані не за чинним правописом, адже низка медій, авторів, видавництв свідомо дотримуються альтернативних правописних правил)

  * має велику базу власних імен (зокрема українських імен, по батькові та прізвищ, неукраїнських імен та прізвищ, українських та закордонних топонімів тощо)

  * синхронізований з КОАТУУ, зокрема містить назви, що з’явилися внаслідок декомунізації

  * має дуже компактну систему позначення відмінювання та тегів для слів, завдяки чому легко додавати нові слова, групувати наявні тощо

  * містить інформацію про деякі рідкісні та розмовні форми, наприклад, нестягнені форми прикметників (гарная), та розмовні інфінітиви (поїхать); щоправда, більшість таких форм вимкнено за уставою, оскільки вони мають обмежену форму застосування й часто створюють зайву омонімію (однак за потреби їх можна ввімкнути)

  * є відкритим проектом (розміщений на github), тож кожен може долучитися до вдосконалення та використовувати його у своїй роботі.

Інші застосування

  * укладання тлумачних, термінологічних, перекладних та інших типів словників (зокрема пошук прикладів вживання)

  * різноманітні мовознавчі дослідження

  * дослідження і розробки у галузі комп’ютерної лінгвістики (зокрема побудова моделей мови, отримання статистичної інформації)

  * довідкові функції та редагування

#### Структура словника

Словник містить три основні частини:
  1. правила зміни суфіксів у парадигмах
  2. слова з прапорцями парадигм та додаткових властивостей
  3. код генерування повних парадигм із сирцевої інформації (1 та 2)

Наприклад, запис тракторист /n20.a.p. – означає, що це відмінюване слово чоловічого роду другої відміни без чергування (і/о), істота, з закінченням -а в родовому відмінку.

### Де застосовується NLP?

Наведена нижче інформація була взята з цього ресурсу : https://evergreens.com.ua/ru/articles/natural-language-processing.html

Якщо вам здається, що ви ніколи не стикалися з NLP, то досить відкрити Google, біля пошукового рядка натиснути на значок мікрофона і сказати: «Окей, гугл ...». І ось пошукова система обробляє потрібний вам запит.

Але ця функція була б недоступна, якби не можливість пристрою зрозуміти природну мову, якою розмовляють люди. Здатність машини обробляти сказане, структурувати отриману інформацію, визначати необхідну відповідь дія і відповідати на мові, зрозумілій користувачеві, і є NLP або Natural Language Processing.

#### Чатбот

NLP стало основою для створення чатботу. Слід сказати, що за допомогою чатботу вирішується проблема завантаженості колл-центрів і приймальних відділень. Наприклад, після впровадження компанією "Київстар" чатботу Зоряна, завантаження операторів значно знизилася. Завдяки великій базі в 12 000 стандартизованих відповідей, бот допомагає з рішенням 70% входять питань. Це підтверджує ефективність використання чатботу для великих компаній.

Багато організацій потребують NLP як в помічника для структурування даних. Адже на сьогодні ще існує завдання оцифровки інформації. І тут ми знову звертаємося до обробника природної мови, який аналізує документацію і класифікує її.Використовують NLP і в медицині для поліпшення обслуговування пацієнтів, ведення медичних карт і пошуку ключових термінів у фаховій літературі. З його допомогою реалізовані лікарі-роботи, які зіставляють симптоми хворих з відповідними діагнозами і відстежують перебіг хвороби.Більш того, можливості обробки даних і прогнозування дозволяють використовувати NLP для запобігання злочинів. Застосовуючи її, поліція може аналізувати злочинну діяльність, обчислювати кодові слова злочинців в рекламі і швидше реагувати щоб уникнути насильства і торгівлі людьми. І це, напевно, найбільш вражаюче застосування NLP на сьогодні.


